---
title: "IS-MRA: Simulation study"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{IS-MRA: Simulation study}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
We start by simulating data. 
```{r setup}
if ("package:ISMRAanalyses" %in% search()) {
  detach("package:ISMRAanalyses", unload = TRUE)
}
library(ISMRAanalyses)
library(sp)

setwd("/home/luc/INLAMRAfiles/INLAMRApaper1/simulations")
# setwd("/home/luc/ISMRAfiles/")
```
The simulated data are based on satellite imagery data obtained in Maharashtra. We first define a rectangular zone in the longitude-latitude projection. Since MODIS data are in the sinusoidal projection, we project the zone we just defined in that coordinate system.
```{r, eval=FALSE}
MaharashtraPolygonEdges <- rbind(c(18.8, 73.2), c(18.8, 73.4), c(18.6, 73.4), c(18.6, 73.2), c(18.8, 73.2))
MaharashtraPolygonEdges <- MaharashtraPolygonEdges[ , 2:1]
MaharashtraPolygon <- sp::SpatialPolygons(Srl = list(sp::Polygons(list(sp::Polygon(coords = MaharashtraPolygonEdges)), ID = "Mumbai")))
raster::crs(MaharashtraPolygon) <- sp::CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0")
MaharashtraPolygonOtherCRS <- sp::spTransform(MaharashtraPolygon, CRSobj = sp::CRS("+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs"))
```
We then import land cover and elevation values from the MODIS and ASTER data files we obtained from EarthData.
```{r, eval=FALSE}
folderForEarthDataFiles <- "/store/luc/rawDataFiles/"
landCoverFiles <- list.files(folderForEarthDataFiles, pattern = "MCD*", full.names = TRUE)
landCoverRasterSinusoidal <- produceLandCover(landCoverFiles, regionPolygon = MaharashtraPolygonOtherCRS)
elevationFiles <- list.files(path = folderForEarthDataFiles, pattern = "*dem.tif", full.names = TRUE)
elevationRasterList <- lapply(elevationFiles, raster::raster)
```
From the imported data, we produce the covariate matrix used in the simulations.
```{r, eval=FALSE}
dayOffset <- 121 # Days are listed numerically, from 1 to 366 (leap year). Day 121 corresponds to April 30.
dayRange <- 27:29 # We extract data for May 27 until May 29.
collectionDatesPOSIX <- as.POSIXct(paste("2012-05-", dayRange, sep = ""))
covariateData <- prepareCovariateDataForISMRA(elevationRasterList, landCoverRasterSinusoidal, collectionDatesPOSIX)
saveRDS(covariateData, file = "data/covariatesForSimulationStudy.rds", compress = TRUE)
```
We are now ready to simulate $1,000$ vectors from a spatiotemporal Gaussian process.
```{r, eval=FALSE}
timepoints <- as.numeric(time(covariateData@time))/(3600*24) # Time is in days

# Parameter values based on real data analysis (cf. table 1 in paper)
# Spatial parameters
spSmoothness <- 1.5
spRange <- 5.66
spScale <- 4.14

# Time parameters
timeSmoothness <- 0.5
timeRange <- 3.60
timeScale <- 1

# Uncorrelated error
errorSD <- 0.5

covfun <- function(spacetimeObject, timepointsVec) {
  spatialDistances <- fields::rdist(spacetimeObject@sp@coords)
  timeDistances <- fields::rdist(timepointsVec)
  GPvecchia::MaternFun(spatialDistances, c(spScale^2, spRange, spSmoothness)) * GPvecchia::MaternFun(timeDistances, c(timeScale^2, timeRange, timeSmoothness))
}

covarStruct <- covfun(covariateData, timepoints) + errorSD^2 * diag(nrow(covariateData@data))

choleskyDecomp <- t(chol(covarStruct))

set.seed(10)

numReplicates <- 1000
fieldValuesWithError <- replicate(n = 1000, expr = as.numeric(choleskyDecomp %*% rnorm(nrow(covarStruct))))
```
We then add fixed effects to the field values, yielding the data we are going to model using the methods we selected for the simulation study. We start by inputting parameter values.
```{r, eval=FALSE}
landCoverEffects <- c(
  Water = 0,
  EvergreenNeedleleaf = 1.09,
  EvergreenBroadleaf = 0.979,
  DeciduousBroadleaf = 1.181,
  MixedForest = 1.087,
  ClosedShrublands = 2.097,
  OpenShrublands = 1.448,
  WoodySavannas = 1.329,
  Savannas = 1.393,
  Grasslands = 1.506,
  PermanentWetlands = 0.705,
  Croplands = 1.588,
  Urban = 1.623,
  CroplandNaturalMosaics = 1.430,
  NonVegetated = 0.667)

# Water is the reference (landCover = 0)
referenceLandCover <- "Water"
referenceLandCoverIndex <- match(referenceLandCover, names(landCoverEffects))

timeEffects <- c(
  May27 = -0.033,
  May28 = -6.319,
  May29 = 1.155) + 0.033 # We're moving the reference day from May 25 to May 27, hence + 0.033
elevationEffect <- -0.001
interceptValue <- 40 # This is arbitrary. It should not affect the analyses.

referenceTimepoint <- "May27"
referenceTimepointIndex <- match(referenceTimepoint, names(timeEffects))
fixedEffectsVec <- c(Intercept = interceptValue, landCoverEffects[-referenceLandCoverIndex], elevation = elevationEffect, timeEffects[-referenceTimepointIndex])
```

We now create the covariate matrix and add covariate effects to the previously-simulated responses.
```{r, eval=FALSE}
landCoverFactor <- factor(covariateData@data$landCover, levels = 0:14, labels = names(landCoverEffects))
timepointsFactor <- factor(timepoints, levels = unique(timepoints), labels = names(timeEffects))

covariateMatrix <- cbind(model.matrix(object = ~ landCover, data = data.frame(landCover = landCoverFactor)), elevation = covariateData@data$elevation, model.matrix(object = ~ time, data = data.frame(time = timepointsFactor))[ , -1]) # We remove the intercept, which has already been created for land cover. Ocean and May 27 are reference categories.
colnames(covariateMatrix) <- replace(colnames(covariateMatrix), which(colnames(covariateMatrix) == "(Intercept)"), "Intercept")

colnames(covariateMatrix) <- gsub(pattern = "landCover", replacement = "", x = colnames(covariateMatrix))
colnames(covariateMatrix) <- gsub(pattern = "time", replacement = "", x = colnames(covariateMatrix))

# Getting the simulated values.

observedValues <- fieldValuesWithError + as.vector(covariateMatrix %*% fixedEffectsVec[colnames(covariateMatrix)])

# Fixing the covariate matrix
landCoverTypes <- names(landCoverEffects)[-referenceLandCoverIndex]
presentLandCoverTypes <- landCoverTypes[sapply(landCoverTypes, function(coverType) !all(covariateMatrix[ , coverType] == 0))]

covariateMatrixNoAbsentLandCovers <- cbind(covariateMatrix[ , -match(landCoverTypes, colnames(covariateMatrix))], covariateMatrix[ , presentLandCoverTypes])

simulatedDatasetsList <- list(
  responses = observedValues,
  covariates = covariateMatrixNoAbsentLandCovers[ , -match("Intercept", colnames(covariateMatrixNoAbsentLandCovers))],
  coords = covariateData@sp@coords,
  time = timepointsFactor,
  timeNumeric = timepoints)

saveRDS(simulatedDatasetsList, file = "data/dataForSimulations.rds", compress = TRUE)
```
As a sanity check, we plot one of the simulated datasets.
```{r, eval=FALSE}
numRasterRows <- numRasterCols <- 48 # Approximate, hard to recover because data were collected in the sinusoidal projection, which does not translate to a perfectly-spaced grid of coordinates in the longitude/latitude projection.
observationsToPlot <- which(simulatedDatasetsList$time == "May28")
fields::quilt.plot(
  x = simulatedDatasetsList$coords[ , "x"][observationsToPlot],
  y = simulatedDatasetsList$coords[ , "y"][observationsToPlot],
  z = simulatedDatasetsList$responses[observationsToPlot],
  nx = numRasterRows, ny = numRasterCols)
```
We now begin the simulations per se.
```{r, eval=FALSE}
simulatedDatasetsList <- readRDS("data/dataForSimulations.rds")
colnames(simulatedDatasetsList$coords) <- c("longitude", "latitude")
simulatedDatasetsList$coords <- as.data.frame(simulatedDatasetsList$coords)

longitudeQuantiles <- quantile(simulatedDatasetsList$coords$longitude, probs = c(0.25, 0.75))
latitudeQuantiles <- quantile(simulatedDatasetsList$coords$latitude, probs = c(0.25, 0.75))

obsIndicesForTraining <-
  !((simulatedDatasetsList$time == "May28") &
      (simulatedDatasetsList$coords$longitude <= longitudeQuantiles[[2]]) &
      (simulatedDatasetsList$coords$longitude >= longitudeQuantiles[[1]]) &
      (simulatedDatasetsList$coords$latitude <= latitudeQuantiles[[2]]) &
      (simulatedDatasetsList$coords$latitude >= latitudeQuantiles[[1]]))

# For Vecchia

# dataCovarianceMatrix <- matrix(0, nrow(simulatedDatasetsList$responses), nrow(simulatedDatasetsList$responses))
# allCoordsWithTime <- cbind(simulatedDatasetsList$coords, time = simulatedDatasetsList$timeNumeric)
#
# for (i in 1:nrow(dataCovarianceMatrix)) {
#   dataCovarianceMatrix[i:nrow(dataCovarianceMatrix), i] <- customCovFct(allCoordsWithTime[i, ], allCoordsWithTime[i:nrow(dataCovarianceMatrix), ])
# }
# dataCovarianceMatrix[upper.tri(dataCovarianceMatrix)] <- dataCovarianceMatrix[lower.tri(dataCovarianceMatrix)]

fittedModelsByDataset <- lapply(
  X = 1:250, # Let's start with 250 datasets...
  FUN = simulationFun,
  responseMatrix = simulatedDatasetsList$responses,
  covariateMatrix = simulatedDatasetsList$covariates,
  coordinatesMatrix = as.matrix(simulatedDatasetsList$coords),
  timeVecNumeric = simulatedDatasetsList$timeNumeric,
  obsIndicesForTraining = obsIndicesForTraining,
  funToFitSPDE = fitSPDE,
  funToFitVecchia = fitVecchia,
  funToFitISMRA = fitISMRA,
  dataCovarianceMatrix = NULL,
  numThreads = 4,
  controlForSPDE = list(mesh.2d.max.n = c(100, 100),
                        mesh.2d.offset = c(5,5)
                        ),
  controlForISMRA = list(control = list(
    Mlon = 2,
    Mlat = 2,
    Mtime = 0,
    numKnotsRes0 = 20,
    numIterOptim = 20,
    tipKnotsThinningRate = 1,
    numValuesForIS = 125
  )),
  saveDirectory = "outputFiles/")
```
(DELETE ME ONCE CHANGE HAS BEEN PROCESSED: I'M FIXING AN ISSUE WITH OUTPUT FROM A PREVIOUS VERSION)
```{r, eval=FALSE}
simulatedDatasetsList <- readRDS("data/dataForSimulations.rds")
colnames(simulatedDatasetsList$coords) <- c("longitude", "latitude")
simulatedDatasetsList$coords <- as.data.frame(simulatedDatasetsList$coords)

longitudeQuantiles <- quantile(simulatedDatasetsList$coords$longitude, probs = c(0.25, 0.75))
latitudeQuantiles <- quantile(simulatedDatasetsList$coords$latitude, probs = c(0.25, 0.75))

obsIndicesForTraining <-
  !((simulatedDatasetsList$time == "May28") &
      (simulatedDatasetsList$coords$longitude <= longitudeQuantiles[[2]]) &
      (simulatedDatasetsList$coords$longitude >= longitudeQuantiles[[1]]) &
      (simulatedDatasetsList$coords$latitude <= latitudeQuantiles[[2]]) &
      (simulatedDatasetsList$coords$latitude >= latitudeQuantiles[[1]]))

# recomputePredictionsForSimOutputs(folderForSimResults = "outputFiles/", patternForFilename = "Dataset.+\\.rds$", coordinatesMatrixTraining = as.matrix(simulatedDatasetsList$coords)[obsIndicesForTraining, ], coordinatesMatrixTest = as.matrix(simulatedDatasetsList$coords)[!obsIndicesForTraining, ], timeVecNumericTraining = simulatedDatasetsList$timeNumeric[obsIndicesForTraining], timeVecNumericTest = simulatedDatasetsList$timeNumeric[!obsIndicesForTraining], covariateMatrixTraining = simulatedDatasetsList$covariates[obsIndicesForTraining, ], covariateMatrixTest = simulatedDatasetsList$covariates[!obsIndicesForTraining, ], responseMatrixTraining = simulatedDatasetsList$responses[obsIndicesForTraining, ], controlForSPDE = do.call("create.SPDE.control", list(mesh.2d.max.n = c(100, 100), mesh.2d.offset = c(5,5))))
stripSPDEobjects(folderForSimulationResults = "outputFiles/", patternForFilename = "Dataset")
refitSPDE(
  folderForSimulationResults = "outputFiles/",
  patternForFilename = "Dataset[[:digit:]]+\\.rds$",
  responseMatrix = simulatedDatasetsList$responses,
  covariateMatrix = simulatedDatasetsList$covariates,
  coordinatesMatrix = as.matrix(simulatedDatasetsList$coords),
  timeVecNumeric = simulatedDatasetsList$timeNumeric,
  funToFitSPDE = fitSPDE,
  controlForSPDE = list(mesh.2d.max.n = c(100, 100),
                        mesh.2d.offset = c(10, 10)
                        ),
  numThreads = 4,
  obsIndicesForTraining = obsIndicesForTraining)
```
We now obtain summaries for prediction accuracy across all simulations for both IS-MRA and SPDE.
```{r}
realFEvalues <- fixedEffectsVec[colnames(simulatedDatasetsList$covariates)]
predictionSummariesByMethod <- analysePredResults(folderForSimResults = "outputFiles/", patternForFilename = "Dataset[[:digit:]]+\\.rds$", simulatedDataList = simulatedDatasetsList, obsIndicesForTraining = obsIndicesForTraining)
mergedTables <- do.call("rbind", predictionSummariesByMethod)
xtable::xtable(mergedTables, digits = 3)
FEerrorsAndSDsByMethod <- analyseParaEsts(folderForSimResults = "outputFiles/", patternForFilename = "Dataset[[:digit:]]+\\.rds$", simulatedDataList = simulatedDatasetsList, obsIndicesForTraining = obsIndicesForTraining, realFEs = realFEvalues)
lapply(FEerrorsAndSDsByMethod, xtable::xtable, digits = 3)
```
In order to reduce the memory footprint, the INLAMRA function does not return the full conditional precision matrix by default. However, in order to show its structure, we can make it return it.
```{r}
ISMRAsampleResultWithQmatrix <- MRAinla::INLAMRA(
  responseVec = simulatedDatasetsList$responses[obsIndicesForTraining, 1],
  spatialCoordMat = simulatedDatasetsList$coords[obsIndicesForTraining, ],
  predSpatialCoordMat = simulatedDatasetsList$coords[!obsIndicesForTraining, ],
  covariateFrame = as.data.frame(simulatedDatasetsList$covariates[obsIndicesForTraining, ]),
  predCovariateFrame = as.data.frame(simulatedDatasetsList$covariates[!obsIndicesForTraining, ]),
  timePOSIXorNumericVec = simulatedDatasetsList$timeNumeric[obsIndicesForTraining],
  predTimePOSIXorNumericVec = simulatedDatasetsList$timeNumeric[!obsIndicesForTraining],
  spatialRangeList = list(log(4), c(log(4), 1)),
  spatialSmoothnessList = list(log(1.5)),
  timeRangeList = list(log(2), c(log(2), 1)),
  timeSmoothnessList = list(log(0.5)),
  scaleList = list(log(3), c(log(3), 1)),
  errorSDlist = list(log(0.5)),
  fixedEffSDlist = list(log(2)),
  control = list(
    Mlon = 2,
    Mlat = 2,
    Mtime = 0,
    numKnotsRes0 = 20,
    numIterOptim = 20,
    tipKnotsThinningRate = 1,
    numValuesForIS = 50,
    returnQmat = TRUE,
    numOpenMPthreads = 4))
library(Matrix)
jpeg("outputFiles/QmatrixExample.jpg", width = 1200, height = 1200)
image(ISMRAsampleResultWithQmatrix$Qmat)
dev.off()
```


